# language-model-evaluator
An evaluation toolkit for large language models like GPT-4, enabling data scientists and engineers to easily estimate API costs, compare different models and prompts, and minimize bias in prompt generation.
